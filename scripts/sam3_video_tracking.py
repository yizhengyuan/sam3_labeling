#!/usr/bin/env python3
"""
SAM 3 Video Tracking Script
ä½¿ç”¨ SAM3 è¿›è¡Œè§†é¢‘ç›®æ ‡è¿½è¸ªï¼Œè¾“å‡º Label Studio å…¼å®¹çš„ JSON æ ¼å¼

æ”¯æŒ:
- CUDA (NVIDIA GPU) - ä½¿ç”¨å®Œæ•´çš„è§†é¢‘è¿½è¸ªå™¨
- MPS (Apple Metal) - ä½¿ç”¨å›¾åƒå¤„ç†å™¨é€å¸§å¤„ç†
- CPU - ä½¿ç”¨å›¾åƒå¤„ç†å™¨é€å¸§å¤„ç†ï¼ˆè¾ƒæ…¢ï¼‰

ä½¿ç”¨å‰è¯·ç¡®ä¿ï¼š
1. å·²å®‰è£… SAM3: pip install -e /path/to/sam3_repo
2. å·²ç™»å½• Hugging Face: huggingface-cli login
3. å·²ç”³è¯· SAM3 æ¨¡å‹è®¿é—®æƒé™: https://huggingface.co/facebook/sam3
"""

import os
import sys
import json
import argparse
import cv2
import numpy as np
import torch
from pathlib import Path
from typing import List, Dict, Any, Optional
from PIL import Image
from collections import defaultdict

# SORT è¿½è¸ªå™¨ä¾èµ–
try:
    from filterpy.kalman import KalmanFilter
    from scipy.optimize import linear_sum_assignment
    SORT_AVAILABLE = True
except ImportError:
    SORT_AVAILABLE = False
    print("âš ï¸ filterpy æˆ– scipy æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å• IoU åŒ¹é…")
    print("   å®‰è£…: pip install filterpy scipy")

# é¢œè‰²åˆ—è¡¨ï¼ˆBGRæ ¼å¼ï¼‰ç”¨äºå¯è§†åŒ–
COLORS = [
    (0, 255, 0),    # ç»¿è‰²
    (255, 0, 0),    # è“è‰²
    (0, 0, 255),    # çº¢è‰²
    (255, 255, 0),  # é’è‰²
    (255, 0, 255),  # ç´«è‰²
    (0, 255, 255),  # é»„è‰²
    (128, 255, 0),  # æµ…ç»¿
    (255, 128, 0),  # æµ…è“
    (128, 0, 255),  # ç²‰è‰²
    (0, 128, 255),  # æ©™è‰²
]

# IoU è·Ÿè¸ªç›¸å…³å‡½æ•°
def calculate_iou(box1, box2):
    """
    è®¡ç®—ä¸¤ä¸ªæ¡†çš„ IoU (Intersection over Union)
    
    Args:
        box1, box2: æ ¼å¼ä¸º (x1, y1, x2, y2) çš„è¾¹ç•Œæ¡†
    
    Returns:
        IoU å€¼ (0-1)
    """
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    
    intersection = max(0, x2 - x1) * max(0, y2 - y1)
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = area1 + area2 - intersection
    
    return intersection / union if union > 0 else 0


def match_detections_to_tracks(detections, active_tracks, iou_threshold=0.3):
    """
    ä½¿ç”¨ IoU åŒ¹é…å½“å‰å¸§çš„æ£€æµ‹ç»“æœä¸å·²æœ‰çš„è½¨è¿¹
    
    Args:
        detections: å½“å‰å¸§çš„æ£€æµ‹ç»“æœ [(label, box, score), ...]
        active_tracks: å·²æœ‰çš„è½¨è¿¹ {track_id: {"label": str, "last_box": box, "last_frame": int}, ...}
        iou_threshold: IoU åŒ¹é…é˜ˆå€¼
    
    Returns:
        matches: [(detection_idx, track_id), ...]
        unmatched_detections: [detection_idx, ...]
    """
    if not detections or not active_tracks:
        return [], list(range(len(detections)))
    
    matches = []
    used_tracks = set()
    unmatched_detections = []
    
    # æŒ‰ç…§æ£€æµ‹åˆ†æ•°ä»é«˜åˆ°ä½æ’åºï¼Œä¼˜å…ˆåŒ¹é…é«˜ç½®ä¿¡åº¦æ£€æµ‹
    sorted_det_indices = sorted(range(len(detections)), 
                                 key=lambda i: detections[i][2], 
                                 reverse=True)
    
    for det_idx in sorted_det_indices:
        det_label, det_box, det_score = detections[det_idx]
        best_iou = 0
        best_track_id = None
        
        for track_id, track_info in active_tracks.items():
            if track_id in used_tracks:
                continue
            # åªåŒ¹é…ç›¸åŒç±»åˆ«çš„ç›®æ ‡
            if track_info["label"] != det_label:
                continue
            
            iou = calculate_iou(det_box, track_info["last_box"])
            if iou > best_iou and iou >= iou_threshold:
                best_iou = iou
                best_track_id = track_id
        
        if best_track_id is not None:
            matches.append((det_idx, best_track_id))
            used_tracks.add(best_track_id)
        else:
            unmatched_detections.append(det_idx)
    
    return matches, unmatched_detections


# ==================== SORT è¿½è¸ªå™¨å®ç° ====================

class KalmanBoxTracker:
    """
    ä½¿ç”¨å¡å°”æ›¼æ»¤æ³¢è¿½è¸ªå•ä¸ªç›®æ ‡çš„è¾¹ç•Œæ¡†
    çŠ¶æ€å‘é‡: [x_center, y_center, area, aspect_ratio, vx, vy, va]
    """
    count = 0
    
    def __init__(self, bbox, label, score=1.0):
        """
        åˆå§‹åŒ–è¿½è¸ªå™¨
        
        Args:
            bbox: [x1, y1, x2, y2] æ ¼å¼çš„è¾¹ç•Œæ¡†
            label: ç›®æ ‡ç±»åˆ«
            score: æ£€æµ‹ç½®ä¿¡åº¦
        """
        if not SORT_AVAILABLE:
            raise ImportError("filterpy not available")
        
        # çŠ¶æ€å‘é‡: [x, y, s, r, vx, vy, vs]
        # x, y: ä¸­å¿ƒåæ ‡, s: é¢ç§¯, r: å®½é«˜æ¯”, v*: é€Ÿåº¦
        self.kf = KalmanFilter(dim_x=7, dim_z=4)
        
        # çŠ¶æ€è½¬ç§»çŸ©é˜µ
        self.kf.F = np.array([
            [1, 0, 0, 0, 1, 0, 0],
            [0, 1, 0, 0, 0, 1, 0],
            [0, 0, 1, 0, 0, 0, 1],
            [0, 0, 0, 1, 0, 0, 0],
            [0, 0, 0, 0, 1, 0, 0],
            [0, 0, 0, 0, 0, 1, 0],
            [0, 0, 0, 0, 0, 0, 1]
        ])
        
        # è§‚æµ‹çŸ©é˜µ
        self.kf.H = np.array([
            [1, 0, 0, 0, 0, 0, 0],
            [0, 1, 0, 0, 0, 0, 0],
            [0, 0, 1, 0, 0, 0, 0],
            [0, 0, 0, 1, 0, 0, 0]
        ])
        
        # æµ‹é‡å™ªå£°
        self.kf.R[2:, 2:] *= 10.
        
        # åæ–¹å·®çŸ©é˜µ
        self.kf.P[4:, 4:] *= 1000.  # é€Ÿåº¦çš„åˆå§‹ä¸ç¡®å®šæ€§
        self.kf.P *= 10.
        
        # è¿‡ç¨‹å™ªå£°
        self.kf.Q[-1, -1] *= 0.01
        self.kf.Q[4:, 4:] *= 0.01
        
        # åˆå§‹åŒ–çŠ¶æ€
        self.kf.x[:4] = self._bbox_to_z(bbox)
        
        self.time_since_update = 0
        self.id = KalmanBoxTracker.count
        KalmanBoxTracker.count += 1
        self.history = []
        self.hits = 0
        self.hit_streak = 0
        self.age = 0
        self.label = label
        self.score = score
    
    @staticmethod
    def _bbox_to_z(bbox):
        """
        å°† [x1, y1, x2, y2] è½¬æ¢ä¸º [x_center, y_center, area, aspect_ratio]
        """
        w = bbox[2] - bbox[0]
        h = bbox[3] - bbox[1]
        x = bbox[0] + w / 2.
        y = bbox[1] + h / 2.
        s = w * h
        r = w / float(h) if h > 0 else 1.0
        return np.array([x, y, s, r]).reshape((4, 1))
    
    @staticmethod
    def _z_to_bbox(z):
        """
        å°† [x_center, y_center, area, aspect_ratio] è½¬æ¢å› [x1, y1, x2, y2]
        """
        w = np.sqrt(z[2] * z[3])
        h = z[2] / w if w > 0 else 0
        return np.array([
            z[0] - w / 2.,
            z[1] - h / 2.,
            z[0] + w / 2.,
            z[1] + h / 2.
        ]).flatten()
    
    def update(self, bbox, score=None):
        """ç”¨æ–°è§‚æµ‹æ›´æ–°çŠ¶æ€"""
        self.time_since_update = 0
        self.history = []
        self.hits += 1
        self.hit_streak += 1
        self.kf.update(self._bbox_to_z(bbox))
        if score is not None:
            self.score = score
    
    def predict(self):
        """é¢„æµ‹ä¸‹ä¸€å¸§ä½ç½®"""
        # é˜²æ­¢é¢ç§¯å˜ä¸ºè´Ÿæ•°
        if (self.kf.x[6] + self.kf.x[2]) <= 0:
            self.kf.x[6] *= 0.0
        
        self.kf.predict()
        self.age += 1
        
        if self.time_since_update > 0:
            self.hit_streak = 0
        self.time_since_update += 1
        
        self.history.append(self._z_to_bbox(self.kf.x))
        return self.history[-1]
    
    def get_state(self):
        """è·å–å½“å‰çŠ¶æ€çš„è¾¹ç•Œæ¡†"""
        return self._z_to_bbox(self.kf.x)


class SORTTracker:
    """
    SORT (Simple Online and Realtime Tracking) è¿½è¸ªå™¨
    
    ç‰¹ç‚¹ï¼š
    - ä½¿ç”¨å¡å°”æ›¼æ»¤æ³¢é¢„æµ‹ç›®æ ‡ä½ç½®
    - ä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•è¿›è¡Œæœ€ä¼˜åŒ¹é…
    - æ”¯æŒå¤šç±»åˆ«ç›®æ ‡è¿½è¸ª
    """
    
    def __init__(self, max_age=30, min_hits=3, iou_threshold=0.3):
        """
        åˆå§‹åŒ–è¿½è¸ªå™¨
        
        Args:
            max_age: ç›®æ ‡ä¸¢å¤±åä¿ç•™çš„æœ€å¤§å¸§æ•°
            min_hits: è¿ç»­å‘½ä¸­å¤šå°‘æ¬¡æ‰ç®—æœ‰æ•ˆè½¨è¿¹
            iou_threshold: IoU åŒ¹é…é˜ˆå€¼
        """
        self.max_age = max_age
        self.min_hits = min_hits
        self.iou_threshold = iou_threshold
        self.trackers = []
        self.frame_count = 0
    
    def update(self, detections):
        """
        æ›´æ–°è¿½è¸ªå™¨
        
        Args:
            detections: [(label, box, score), ...] å½“å‰å¸§çš„æ£€æµ‹ç»“æœ
                        box æ ¼å¼: (x1, y1, x2, y2)
        
        Returns:
            tracks: [(label, box, track_id, score), ...] å½“å‰å¸§çš„è¿½è¸ªç»“æœ
        """
        self.frame_count += 1
        
        # é¢„æµ‹æ‰€æœ‰ç°æœ‰è½¨è¿¹çš„æ–°ä½ç½®
        trks = np.zeros((len(self.trackers), 5))
        to_del = []
        for t, trk in enumerate(self.trackers):
            pos = trk.predict()
            trks[t, :] = [pos[0], pos[1], pos[2], pos[3], 0]
            if np.any(np.isnan(pos)):
                to_del.append(t)
        
        # åˆ é™¤æ— æ•ˆè½¨è¿¹
        for t in reversed(to_del):
            self.trackers.pop(t)
        
        # åŒ¹é…æ£€æµ‹ç»“æœå’Œè½¨è¿¹
        matched, unmatched_dets, unmatched_trks = self._associate_detections_to_trackers(
            detections, self.trackers, self.iou_threshold
        )
        
        # æ›´æ–°åŒ¹é…çš„è½¨è¿¹
        for d, t in matched:
            label, box, score = detections[d]
            self.trackers[t].update(box, score)
        
        # ä¸ºæœªåŒ¹é…çš„æ£€æµ‹åˆ›å»ºæ–°è½¨è¿¹
        for d in unmatched_dets:
            label, box, score = detections[d]
            trk = KalmanBoxTracker(box, label, score)
            self.trackers.append(trk)
        
        # è¿”å›æœ‰æ•ˆè½¨è¿¹
        ret = []
        i = len(self.trackers)
        for trk in reversed(self.trackers):
            d = trk.get_state()
            # åªè¿”å›è¿‘æœŸæœ‰æ›´æ–°ä¸”è¾¾åˆ°æœ€å°å‘½ä¸­æ¬¡æ•°çš„è½¨è¿¹
            if (trk.time_since_update < 1) and \
               (trk.hit_streak >= self.min_hits or self.frame_count <= self.min_hits):
                ret.append((trk.label, tuple(d), trk.id, trk.score))
            i -= 1
            # åˆ é™¤é•¿æ—¶é—´æœªæ›´æ–°çš„è½¨è¿¹
            if trk.time_since_update > self.max_age:
                self.trackers.pop(i)
        
        return ret
    
    def _associate_detections_to_trackers(self, detections, trackers, iou_threshold):
        """
        ä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•å°†æ£€æµ‹ç»“æœä¸è½¨è¿¹å…³è”
        
        Returns:
            matches: [[det_idx, trk_idx], ...]
            unmatched_detections: [det_idx, ...]
            unmatched_trackers: [trk_idx, ...]
        """
        if len(trackers) == 0:
            return np.empty((0, 2), dtype=int), np.arange(len(detections)), np.empty((0, 5), dtype=int)
        
        if len(detections) == 0:
            return np.empty((0, 2), dtype=int), np.empty((0, 5), dtype=int), np.arange(len(trackers))
        
        # æ„å»º IoU çŸ©é˜µ
        iou_matrix = np.zeros((len(detections), len(trackers)), dtype=np.float32)
        
        for d, det in enumerate(detections):
            det_label, det_box, det_score = det
            for t, trk in enumerate(trackers):
                # åªåŒ¹é…ç›¸åŒç±»åˆ«
                if det_label == trk.label:
                    iou_matrix[d, t] = calculate_iou(det_box, trk.get_state())
        
        # ä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•æ±‚è§£æœ€ä¼˜åŒ¹é…
        if min(iou_matrix.shape) > 0:
            # è½¬æ¢ä¸ºä»£ä»·çŸ©é˜µï¼ˆ1 - IoUï¼‰
            row_indices, col_indices = linear_sum_assignment(-iou_matrix)
            matched_indices = np.array(list(zip(row_indices, col_indices)))
        else:
            matched_indices = np.empty((0, 2), dtype=int)
        
        # æ‰¾å‡ºæœªåŒ¹é…çš„æ£€æµ‹
        unmatched_detections = []
        for d in range(len(detections)):
            if d not in matched_indices[:, 0] if len(matched_indices) > 0 else True:
                unmatched_detections.append(d)
        
        # æ‰¾å‡ºæœªåŒ¹é…çš„è½¨è¿¹
        unmatched_trackers = []
        for t in range(len(trackers)):
            if t not in matched_indices[:, 1] if len(matched_indices) > 0 else True:
                unmatched_trackers.append(t)
        
        # è¿‡æ»¤æ‰ä½ IoU çš„åŒ¹é…
        matches = []
        for m in matched_indices:
            if iou_matrix[m[0], m[1]] < iou_threshold:
                unmatched_detections.append(m[0])
                unmatched_trackers.append(m[1])
            else:
                matches.append(m.reshape(1, 2))
        
        if len(matches) == 0:
            matches = np.empty((0, 2), dtype=int)
        else:
            matches = np.concatenate(matches, axis=0)
        
        return matches, np.array(unmatched_detections), np.array(unmatched_trackers)


# æ£€æŸ¥ SAM3 æ˜¯å¦å¯ç”¨
SAM3_AVAILABLE = False
SAM3_VIDEO_AVAILABLE = False

try:
    from sam3.model_builder import build_sam3_image_model, build_sam3_video_predictor
    from sam3.model.sam3_image_processor import Sam3Processor
    SAM3_AVAILABLE = True
    
    # æ£€æŸ¥æ˜¯å¦æ”¯æŒè§†é¢‘è¿½è¸ªï¼ˆéœ€è¦ CUDAï¼‰
    if torch.cuda.is_available():
        SAM3_VIDEO_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ SAM3 æœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·è¿è¡Œ: pip install -e /path/to/sam3_repo")


def run_video_tracking_cuda(
    video_path: str,
    text_prompt: str,
    output_path: str,
    sample_rate: int = 1
):
    """
    ä½¿ç”¨ CUDA çš„å®Œæ•´è§†é¢‘è¿½è¸ªï¼ˆéœ€è¦ NVIDIA GPUï¼‰
    """
    from sam3.visualization_utils import prepare_masks_for_visualization
    
    print(f"ğŸ¬ åŠ è½½è§†é¢‘: {video_path}")
    
    # è·å–è§†é¢‘ä¿¡æ¯
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    cap.release()
    
    print(f"   å¸§ç‡: {fps}, æ€»å¸§æ•°: {frame_count}")
    
    gpus_to_use = list(range(torch.cuda.device_count()))
    print(f"ğŸ–¥ï¸ ä½¿ç”¨ GPU: {gpus_to_use}")
    
    # æ„å»º SAM3 è§†é¢‘é¢„æµ‹å™¨
    print("ğŸ”§ åŠ è½½ SAM3 æ¨¡å‹...")
    predictor = build_sam3_video_predictor(gpus_to_use=gpus_to_use)
    
    # å¼€å§‹ä¼šè¯
    print("ğŸ“¹ å¼€å§‹è§†é¢‘ä¼šè¯...")
    response = predictor.handle_request(
        request=dict(
            type="start_session",
            resource_path=video_path,
        )
    )
    session_id = response["session_id"]
    
    # æ·»åŠ æ–‡æœ¬æç¤º
    print(f"ğŸ·ï¸ æ·»åŠ æ–‡æœ¬æç¤º: '{text_prompt}'")
    response = predictor.handle_request(
        request=dict(
            type="add_prompt",
            session_id=session_id,
            frame_index=0,
            text=text_prompt,
        )
    )
    
    # ä¼ æ’­è¿½è¸ª
    print("ğŸ”„ ä¼ æ’­è¿½è¸ªåˆ°æ‰€æœ‰å¸§...")
    outputs_per_frame = {}
    for response in predictor.handle_stream_request(
        request=dict(
            type="propagate_in_video",
            session_id=session_id,
        )
    ):
        outputs_per_frame[response["frame_index"]] = response["outputs"]
    
    print(f"   è¿½è¸ªå®Œæˆï¼Œå…± {len(outputs_per_frame)} å¸§")
    
    # å…³é—­ä¼šè¯
    predictor.handle_request(
        request=dict(
            type="close_session",
            session_id=session_id,
        )
    )
    predictor.shutdown()
    
    # è½¬æ¢ä¸º Label Studio æ ¼å¼
    return convert_outputs_to_label_studio(
        outputs_per_frame, video_path, fps, text_prompt, output_path
    )


def draw_box_on_frame(frame, x1, y1, x2, y2, label, obj_id, color):
    """åœ¨å¸§ä¸Šç»˜åˆ¶è¾¹ç•Œæ¡†"""
    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
    
    # ç»˜åˆ¶æ ‡ç­¾èƒŒæ™¯
    text = f"{label} #{obj_id}"
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.5
    thickness = 1
    (text_width, text_height), baseline = cv2.getTextSize(text, font, font_scale, thickness)
    
    cv2.rectangle(frame, (x1, y1 - text_height - 6), (x1 + text_width + 6, y1), color, -1)
    cv2.putText(frame, text, (x1 + 3, y1 - 3), font, font_scale, (255, 255, 255), thickness)
    
    return frame


def run_video_tracking_mps_cpu(
    video_path: str,
    text_prompts: List[str],  # æ”¹ä¸ºæ”¯æŒå¤šä¸ªæç¤ºè¯
    output_path: str,
    device: str = "mps",
    sample_rate: int = 5,
    checkpoint_path: str = None,
    generate_video: bool = True,  # æ˜¯å¦ç”Ÿæˆæ ‡æ³¨è§†é¢‘
    confidence_threshold: float = 0.3,
    iou_threshold: float = 0.15,  # IoU åŒ¹é…é˜ˆå€¼
    debug: bool = False,  # æ˜¯å¦æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
    use_sort: bool = True,  # æ˜¯å¦ä½¿ç”¨ SORT è¿½è¸ªå™¨
    max_age: int = None,  # SORT: ç›®æ ‡ä¸¢å¤±åä¿ç•™çš„æœ€å¤§å¸§æ•°
    min_hits: int = 3  # SORT: è¿ç»­å‘½ä¸­å¤šå°‘æ¬¡æ‰ç®—æœ‰æ•ˆè½¨è¿¹
):
    """
    ä½¿ç”¨ MPS æˆ– CPU çš„é€å¸§å¤„ç†ï¼ˆé€‚ç”¨äº Mac æˆ–æ—  GPU ç¯å¢ƒï¼‰
    æ”¯æŒå¤šä¸ªæ–‡æœ¬æç¤ºï¼Œæ¯ä¸ªç›®æ ‡ç‹¬ç«‹æ ‡æ³¨
    
    è¿½è¸ªæ¨¡å¼:
    - SORT è¿½è¸ªå™¨ (use_sort=True): ä½¿ç”¨å¡å°”æ›¼æ»¤æ³¢ + åŒˆç‰™åˆ©ç®—æ³•ï¼ŒID æ›´ç¨³å®š
    - ç®€å• IoU åŒ¹é… (use_sort=False): åŸå§‹æ–¹å¼ï¼Œé€‚ç”¨äº filterpy æœªå®‰è£…çš„æƒ…å†µ
    """
    print(f"ğŸ¬ åŠ è½½è§†é¢‘: {video_path}")
    
    # è·å–è§†é¢‘ä¿¡æ¯
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    print(f"   å¸§ç‡: {fps}, æ€»å¸§æ•°: {frame_count}, åˆ†è¾¨ç‡: {width}x{height}")
    print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"ğŸ“Š é‡‡æ ·ç‡: æ¯ {sample_rate} å¸§å¤„ç†ä¸€æ¬¡")
    print(f"ğŸ·ï¸ æ£€æµ‹ç›®æ ‡: {text_prompts}")
    
    # å†³å®šä½¿ç”¨å“ªç§è¿½è¸ªæ¨¡å¼
    use_sort_tracker = use_sort and SORT_AVAILABLE
    if use_sort and not SORT_AVAILABLE:
        print("âš ï¸ SORT è¿½è¸ªå™¨ä¸å¯ç”¨ï¼ˆfilterpy æœªå®‰è£…ï¼‰ï¼Œä½¿ç”¨ç®€å• IoU åŒ¹é…")
    
    if use_sort_tracker:
        # åˆå§‹åŒ– SORT è¿½è¸ªå™¨
        if max_age is None:
            max_age = int(fps * 2)  # é»˜è®¤ä¸¢å¤± 2 ç§’ååˆ é™¤è½¨è¿¹
        sort_tracker = SORTTracker(
            max_age=max_age,
            min_hits=min_hits,
            iou_threshold=iou_threshold
        )
        print(f"ğŸ”„ ä½¿ç”¨ SORT è¿½è¸ªå™¨ (å¡å°”æ›¼æ»¤æ³¢ + åŒˆç‰™åˆ©ç®—æ³•)")
        print(f"   å‚æ•°: max_age={max_age}, min_hits={min_hits}, iou_threshold={iou_threshold}")
    else:
        print(f"ğŸ”„ ä½¿ç”¨ç®€å• IoU åŒ¹é…è¿½è¸ª")
    
    # æ„å»º SAM3 å›¾åƒæ¨¡å‹
    print("ğŸ”§ åŠ è½½ SAM3 æ¨¡å‹...")
    if checkpoint_path:
        print(f"   ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {checkpoint_path}")
        model = build_sam3_image_model(device=device, checkpoint_path=checkpoint_path, load_from_HF=False)
    else:
        model = build_sam3_image_model(device=device)
    processor = Sam3Processor(model, device=device, confidence_threshold=confidence_threshold)
    
    # å‡†å¤‡è¾“å‡ºè§†é¢‘
    video_output_path = None
    video_writer = None
    temp_video_path = None
    if generate_video:
        video_output_path = output_path.replace('.json', '_annotated.mp4')
        # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ï¼Œä¹‹åç”¨ ffmpeg é‡æ–°ç¼–ç ä»¥è·å¾—æ›´å¥½çš„å…¼å®¹æ€§
        temp_video_path = output_path.replace('.json', '_temp.avi')
        # ä½¿ç”¨ XVID ç¼–ç ï¼Œå…¼å®¹æ€§æ›´å¥½
        fourcc = cv2.VideoWriter_fourcc(*'XVID')
        video_writer = cv2.VideoWriter(temp_video_path, fourcc, fps, (width, height))
    
    # å­˜å‚¨æ‰€æœ‰æ£€æµ‹ç»“æœ
    # ç»“æ„: {track_id: {"label": str, "global_id": int, "frames": {frame_idx: box_data}}}
    all_results = {}
    
    # ç®€å• IoU åŒ¹é…æ¨¡å¼çš„å˜é‡
    active_tracks = {}  # {track_id: {"label": str, "last_box": (x1,y1,x2,y2), "last_frame": int}}
    next_track_id = 0
    max_missing_frames = int(fps * 2)  # æœ€å¤šå…è®¸ä¸¢å¤± 2 ç§’
    
    # å¤„ç†è§†é¢‘å¸§
    print(f"ğŸ”„ å¤„ç†è§†é¢‘å¸§...")
    
    frame_idx = 0
    processed_count = 0
    
    # å½“å‰å¸§çš„æ ‡æ³¨ï¼ˆç”¨äºè§†é¢‘ç»˜åˆ¶ï¼‰
    current_frame_annotations = []
    
    # é‡æ–°æ‰“å¼€è§†é¢‘
    cap = cv2.VideoCapture(video_path)
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        
        frame_annotations = []  # å½“å‰å¸§çš„æ‰€æœ‰æ ‡æ³¨
        
        # é‡‡æ ·å¸§è¿›è¡Œæ£€æµ‹
        if frame_idx % sample_rate == 0:
            # è½¬æ¢ä¸º PIL Image
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            
            # æ”¶é›†å½“å‰å¸§æ‰€æœ‰æ£€æµ‹ç»“æœ
            current_detections = []  # [(label, box, score), ...]
            
            # å¯¹æ¯ä¸ªæ–‡æœ¬æç¤ºè¿›è¡Œæ£€æµ‹
            for text_prompt in text_prompts:
                # è®¾ç½®å›¾åƒ
                state = processor.set_image(pil_image)
                
                # ä½¿ç”¨æ–‡æœ¬æç¤ºè¿›è¡Œæ£€æµ‹
                output = processor.set_text_prompt(state=state, prompt=text_prompt)
                
                boxes = output.get("boxes", [])
                scores = output.get("scores", [])
                
                # æ”¶é›†æ£€æµ‹ç»“æœ
                for i in range(len(boxes)):
                    box = boxes[i]
                    score = scores[i] if i < len(scores) else 0.5
                    
                    if score < confidence_threshold:
                        continue
                    
                    # box æ ¼å¼: [x1, y1, x2, y2] åƒç´ åæ ‡
                    x1, y1, x2, y2 = box.cpu().numpy()
                    current_detections.append((text_prompt, (float(x1), float(y1), float(x2), float(y2)), float(score)))
            
            # ========== è¿½è¸ªå¤„ç† ==========
            if use_sort_tracker:
                # ä½¿ç”¨ SORT è¿½è¸ªå™¨
                tracks = sort_tracker.update(current_detections)
                
                if debug and tracks:
                    print(f"   [å¸§ {frame_idx}] æ£€æµ‹åˆ° {len(current_detections)} ä¸ªç›®æ ‡, "
                          f"è¿½è¸ªåˆ° {len(tracks)} ä¸ª")
                
                for label, box, track_id, score in tracks:
                    x1, y1, x2, y2 = box
                    
                    # ç¡®ä¿ç»“æœè®°å½•å­˜åœ¨
                    if track_id not in all_results:
                        all_results[track_id] = {
                            "label": label,
                            "global_id": track_id,
                            "frames": {}
                        }
                    
                    # è½¬æ¢ä¸º Label Studio æ ¼å¼ (ç™¾åˆ†æ¯” 0-100)
                    box_data = {
                        "x": x1 / width * 100,
                        "y": y1 / height * 100,
                        "width": (x2 - x1) / width * 100,
                        "height": (y2 - y1) / height * 100,
                        "score": score,
                        "time": frame_idx / fps,
                        "pixel_box": (int(x1), int(y1), int(x2), int(y2))
                    }
                    all_results[track_id]["frames"][frame_idx] = box_data
                    
                    # æ·»åŠ åˆ°å½“å‰å¸§çš„æ ‡æ³¨åˆ—è¡¨
                    frame_annotations.append({
                        "label": label,
                        "obj_id": track_id,
                        "pixel_box": (int(x1), int(y1), int(x2), int(y2)),
                        "color": COLORS[track_id % len(COLORS)]
                    })
            else:
                # ä½¿ç”¨ç®€å• IoU åŒ¹é…
                matches, unmatched = match_detections_to_tracks(
                    current_detections, active_tracks, iou_threshold
                )
                
                if debug and (matches or unmatched):
                    print(f"   [å¸§ {frame_idx}] æ£€æµ‹åˆ° {len(current_detections)} ä¸ªç›®æ ‡, "
                          f"åŒ¹é… {len(matches)} ä¸ª, æ–°å¢ {len(unmatched)} ä¸ª")
                
                # æ›´æ–°åŒ¹é…çš„è½¨è¿¹
                for det_idx, track_id in matches:
                    label, box, score = current_detections[det_idx]
                    x1, y1, x2, y2 = box
                    
                    active_tracks[track_id]["last_box"] = box
                    active_tracks[track_id]["last_frame"] = frame_idx
                    
                    box_data = {
                        "x": x1 / width * 100,
                        "y": y1 / height * 100,
                        "width": (x2 - x1) / width * 100,
                        "height": (y2 - y1) / height * 100,
                        "score": score,
                        "time": frame_idx / fps,
                        "pixel_box": (int(x1), int(y1), int(x2), int(y2))
                    }
                    all_results[track_id]["frames"][frame_idx] = box_data
                    
                    frame_annotations.append({
                        "label": label,
                        "obj_id": all_results[track_id]["global_id"],
                        "pixel_box": (int(x1), int(y1), int(x2), int(y2)),
                        "color": COLORS[all_results[track_id]["global_id"] % len(COLORS)]
                    })
                
                # ä¸ºæœªåŒ¹é…çš„æ£€æµ‹åˆ›å»ºæ–°è½¨è¿¹
                for det_idx in unmatched:
                    label, box, score = current_detections[det_idx]
                    x1, y1, x2, y2 = box
                    
                    track_id = next_track_id
                    next_track_id += 1
                    
                    active_tracks[track_id] = {
                        "label": label,
                        "last_box": box,
                        "last_frame": frame_idx
                    }
                    
                    all_results[track_id] = {
                        "label": label,
                        "global_id": track_id,
                        "frames": {}
                    }
                    
                    box_data = {
                        "x": x1 / width * 100,
                        "y": y1 / height * 100,
                        "width": (x2 - x1) / width * 100,
                        "height": (y2 - y1) / height * 100,
                        "score": score,
                        "time": frame_idx / fps,
                        "pixel_box": (int(x1), int(y1), int(x2), int(y2))
                    }
                    all_results[track_id]["frames"][frame_idx] = box_data
                    
                    frame_annotations.append({
                        "label": label,
                        "obj_id": track_id,
                        "pixel_box": (int(x1), int(y1), int(x2), int(y2)),
                        "color": COLORS[track_id % len(COLORS)]
                    })
                
                # æ¸…ç†é•¿æ—¶é—´æœªæ›´æ–°çš„è½¨è¿¹
                tracks_to_remove = [
                    tid for tid, info in active_tracks.items()
                    if frame_idx - info["last_frame"] > max_missing_frames
                ]
                for tid in tracks_to_remove:
                    del active_tracks[tid]
            
            # ä¿å­˜å½“å‰å¸§æ ‡æ³¨
            current_frame_annotations = frame_annotations.copy()
            
            processed_count += 1
            if processed_count % 10 == 0:
                if use_sort_tracker:
                    print(f"   å·²å¤„ç† {processed_count} å¸§ï¼Œæ´»è·ƒè½¨è¿¹: {len(sort_tracker.trackers)}")
                else:
                    print(f"   å·²å¤„ç† {processed_count} å¸§ï¼Œæ´»è·ƒè½¨è¿¹: {len(active_tracks)}")
        else:
            # éé‡‡æ ·å¸§ï¼šä½¿ç”¨ä¸Šä¸€æ¬¡çš„æ ‡æ³¨ï¼ˆæˆ–è€…ä½¿ç”¨ SORT é¢„æµ‹ï¼‰
            if use_sort_tracker and SORT_AVAILABLE:
                # SORT æ¨¡å¼ï¼šå¯ä»¥ä½¿ç”¨å¡å°”æ›¼æ»¤æ³¢é¢„æµ‹ä½ç½®
                for trk in sort_tracker.trackers:
                    if trk.time_since_update == 0:  # åªæ˜¾ç¤ºæœ€è¿‘æ›´æ–°è¿‡çš„è½¨è¿¹
                        pred_box = trk.get_state()
                        x1, y1, x2, y2 = pred_box
                        frame_annotations.append({
                            "label": trk.label,
                            "obj_id": trk.id,
                            "pixel_box": (int(x1), int(y1), int(x2), int(y2)),
                            "color": COLORS[trk.id % len(COLORS)]
                        })
            else:
                # ç®€å•æ¨¡å¼ï¼šä½¿ç”¨ä¸Šä¸€å¸§çš„æ ‡æ³¨
                frame_annotations = current_frame_annotations.copy()
        
        # ç”Ÿæˆæ ‡æ³¨è§†é¢‘å¸§
        if video_writer is not None:
            # åœ¨å¸§ä¸Šç»˜åˆ¶æ‰€æœ‰æ ‡æ³¨
            for ann in frame_annotations:
                x1, y1, x2, y2 = ann["pixel_box"]
                frame = draw_box_on_frame(
                    frame, x1, y1, x2, y2,
                    ann["label"], ann["obj_id"], ann["color"]
                )
            
            # æ˜¾ç¤ºå¸§å·å’Œè¿½è¸ªæ¨¡å¼
            tracker_mode = "SORT" if use_sort_tracker else "IoU"
            cv2.putText(frame, f"Frame: {frame_idx} [{tracker_mode}]", (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
            
            video_writer.write(frame)
        
        frame_idx += 1
    
    cap.release()
    if video_writer is not None:
        video_writer.release()
        
        # ä½¿ç”¨ ffmpeg é‡æ–°ç¼–ç ä¸º H.264 MP4ï¼Œè·å¾—æ›´å¥½çš„å…¼å®¹æ€§
        if temp_video_path and os.path.exists(temp_video_path):
            print("ğŸ”„ æ­£åœ¨ä¼˜åŒ–è§†é¢‘ç¼–ç ...")
            import subprocess
            try:
                # ä½¿ç”¨ ffmpeg è½¬æ¢ä¸º H.264 ç¼–ç 
                cmd = [
                    'ffmpeg', '-y', '-i', temp_video_path,
                    '-c:v', 'libx264', '-preset', 'fast', '-crf', '23',
                    '-pix_fmt', 'yuv420p',  # ç¡®ä¿å…¼å®¹æ€§
                    video_output_path
                ]
                subprocess.run(cmd, capture_output=True, check=True)
                os.remove(temp_video_path)  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                print("   è§†é¢‘ç¼–ç ä¼˜åŒ–å®Œæˆ")
            except (subprocess.CalledProcessError, FileNotFoundError):
                # å¦‚æœ ffmpeg ä¸å¯ç”¨ï¼Œç›´æ¥é‡å‘½åä¸´æ—¶æ–‡ä»¶
                print("   ffmpeg ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸå§‹ç¼–ç ")
                import shutil
                shutil.move(temp_video_path, video_output_path)
    
    print(f"   å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {processed_count} å¸§")
    
    # è½¬æ¢ä¸º Label Studio æ ¼å¼
    ls_results = []
    for obj_key, obj_data in all_results.items():
        frames_data = obj_data["frames"]
        if not frames_data:
            continue
        
        sequence = []
        for fidx, data in sorted(frames_data.items()):
            sequence.append({
                "frame": fidx,
                "x": data["x"],
                "y": data["y"],
                "width": data["width"],
                "height": data["height"],
                "rotation": 0,
                "time": data["time"],
                "enabled": True
            })
        
        if sequence:
            ls_results.append({
                "from_name": "box",
                "to_name": "video",
                "type": "videorectangle",
                "value": {
                    "sequence": sequence,
                    "labels": [obj_data["label"]]
                },
                "id": f"obj_{obj_data['global_id']}"
            })
    
    # ä¿å­˜ JSON ç»“æœ
    tracker_version = "SAM3-SORT" if use_sort_tracker else "SAM3-IoU"
    output_data = [{
        "data": {
            "video": f"/data/local-files/?d={os.path.basename(video_path)}"
        },
        "predictions": [{
            "result": ls_results,
            "model_version": tracker_version
        }]
    }]
    
    os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… JSON ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    print(f"   å…±è¿½è¸ªåˆ° {len(ls_results)} ä¸ªç›®æ ‡")
    
    if video_output_path:
        print(f"âœ… æ ‡æ³¨è§†é¢‘å·²ä¿å­˜åˆ°: {video_output_path}")


def convert_outputs_to_label_studio(
    outputs_per_frame: Dict,
    video_path: str,
    fps: float,
    text_prompt: str,
    output_path: str
):
    """å°† SAM3 è¾“å‡ºè½¬æ¢ä¸º Label Studio æ ¼å¼"""
    from sam3.visualization_utils import prepare_masks_for_visualization
    
    print("ğŸ“ è½¬æ¢ä¸º Label Studio æ ¼å¼...")
    formatted_outputs = prepare_masks_for_visualization(outputs_per_frame)
    
    all_results = {}  # {obj_id: [sequence]}
    
    for frame_idx, frame_output in formatted_outputs.items():
        if frame_output is None:
            continue
        
        boxes = frame_output.get("boxes", [])
        scores = frame_output.get("scores", [])
        obj_ids = frame_output.get("obj_ids", [])
        
        for i, obj_id in enumerate(obj_ids):
            if i < len(boxes) and boxes[i] is not None:
                box = boxes[i]
                x = float(box[0]) * 100
                y = float(box[1]) * 100
                width = float(box[2] - box[0]) * 100
                height = float(box[3] - box[1]) * 100
                score = float(scores[i]) if i < len(scores) else 1.0
                
                str_obj_id = f"obj_{obj_id}"
                if str_obj_id not in all_results:
                    all_results[str_obj_id] = []
                
                all_results[str_obj_id].append({
                    "frame": frame_idx,
                    "x": x,
                    "y": y,
                    "width": width,
                    "height": height,
                    "rotation": 0,
                    "time": frame_idx / fps,
                    "enabled": True
                })
    
    ls_results = []
    for obj_id, sequence in all_results.items():
        if sequence:
            ls_results.append({
                "from_name": "box",
                "to_name": "video",
                "type": "videorectangle",
                "value": {
                    "sequence": sorted(sequence, key=lambda x: x["frame"]),
                    "labels": [text_prompt]
                },
                "id": obj_id
            })
    
    output_data = [{
        "data": {
            "video": f"/data/local-files/?d={os.path.basename(video_path)}"
        },
        "predictions": [{
            "result": ls_results,
            "model_version": "SAM3"
        }]
    }]
    
    os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    print(f"   å…±è¿½è¸ªåˆ° {len(ls_results)} ä¸ªç›®æ ‡")


def main():
    parser = argparse.ArgumentParser(
        description="SAM3 è§†é¢‘è¿½è¸ª - ä½¿ç”¨æ–‡æœ¬æç¤ºè¿½è¸ªè§†é¢‘ä¸­çš„ç›®æ ‡"
    )
    parser.add_argument(
        "video_path",
        help="è¾“å…¥è§†é¢‘è·¯å¾„ (MP4 æˆ– JPEG å¸§ç›®å½•)"
    )
    parser.add_argument(
        "--text", "-t",
        required=True,
        nargs="+",  # æ”¯æŒå¤šä¸ªæ–‡æœ¬æç¤º
        help="æ–‡æœ¬æç¤ºï¼Œæè¿°è¦è¿½è¸ªçš„ç›®æ ‡ï¼Œå¯ä»¥æŒ‡å®šå¤šä¸ª (å¦‚ -t car 'traffic sign')"
    )
    parser.add_argument(
        "--output", "-o",
        default="SAM3_output/tracking_result.json",
        help="è¾“å‡º JSON è·¯å¾„ (é»˜è®¤: SAM3_output/tracking_result.json)"
    )
    parser.add_argument(
        "--device", "-d",
        default=None,
        choices=["cuda", "cpu", "mps"],
        help="è®¡ç®—è®¾å¤‡ (é»˜è®¤: è‡ªåŠ¨é€‰æ‹©)"
    )
    parser.add_argument(
        "--sample-rate", "-s",
        type=int,
        default=5,
        help="é‡‡æ ·ç‡ï¼Œæ¯ N å¸§å¤„ç†ä¸€æ¬¡ (ä»…ç”¨äº MPS/CPU æ¨¡å¼ï¼Œé»˜è®¤: 5)"
    )
    parser.add_argument(
        "--checkpoint", "-c",
        default=None,
        help="æœ¬åœ°æ¨¡å‹ checkpoint è·¯å¾„ (å¦‚ checkpoints/sam3/sam3.pt)"
    )
    parser.add_argument(
        "--no-video",
        action="store_true",
        help="ä¸ç”Ÿæˆæ ‡æ³¨è§†é¢‘ï¼Œåªè¾“å‡º JSON"
    )
    parser.add_argument(
        "--confidence", 
        type=float,
        default=0.3,
        help="ç½®ä¿¡åº¦é˜ˆå€¼ (é»˜è®¤: 0.3)"
    )
    parser.add_argument(
        "--iou-threshold",
        type=float,
        default=0.15,
        help="IoU åŒ¹é…é˜ˆå€¼ï¼Œè¶Šä½è¶Šå®¹æ˜“åŒ¹é…åŒä¸€ç›®æ ‡ (é»˜è®¤: 0.15)"
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†çš„è·Ÿè¸ªè°ƒè¯•ä¿¡æ¯"
    )
    parser.add_argument(
        "--no-sort",
        action="store_true",
        help="ç¦ç”¨ SORT è¿½è¸ªå™¨ï¼Œä½¿ç”¨ç®€å• IoU åŒ¹é…ï¼ˆä¸æ¨èï¼‰"
    )
    parser.add_argument(
        "--max-age",
        type=int,
        default=None,
        help="SORT: ç›®æ ‡ä¸¢å¤±åä¿ç•™çš„æœ€å¤§å¸§æ•°ï¼ˆé»˜è®¤: fps * 2ï¼‰"
    )
    parser.add_argument(
        "--min-hits",
        type=int,
        default=3,
        help="SORT: è¿ç»­å‘½ä¸­å¤šå°‘æ¬¡æ‰ç®—æœ‰æ•ˆè½¨è¿¹ï¼ˆé»˜è®¤: 3ï¼‰"
    )
    
    args = parser.parse_args()
    
    if not SAM3_AVAILABLE:
        print("âŒ SAM3 ä¸å¯ç”¨ï¼Œæ— æ³•è¿è¡Œ")
        sys.exit(1)
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.video_path):
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {args.video_path}")
        sys.exit(1)
    
    # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
    if args.device is None:
        if torch.cuda.is_available():
            args.device = "cuda"
        elif torch.backends.mps.is_available():
            args.device = "mps"
        else:
            args.device = "cpu"
    
    # æ ¹æ®è®¾å¤‡é€‰æ‹©å¤„ç†æ–¹å¼
    if args.device == "cuda" and SAM3_VIDEO_AVAILABLE:
        run_video_tracking_cuda(
            video_path=args.video_path,
            text_prompt=args.text[0],  # CUDA æ¨¡å¼æš‚æ—¶åªæ”¯æŒå•ä¸ªæç¤º
            output_path=args.output,
            sample_rate=args.sample_rate
        )
    else:
        print(f"âš ï¸ ä½¿ç”¨é€å¸§å¤„ç†æ¨¡å¼ (è®¾å¤‡: {args.device})")
        run_video_tracking_mps_cpu(
            video_path=args.video_path,
            text_prompts=args.text,  # æ”¯æŒå¤šä¸ªæç¤º
            output_path=args.output,
            device=args.device,
            sample_rate=args.sample_rate,
            checkpoint_path=args.checkpoint,
            generate_video=not args.no_video,
            confidence_threshold=args.confidence,
            iou_threshold=args.iou_threshold,
            debug=args.debug,
            use_sort=not args.no_sort,
            max_age=args.max_age,
            min_hits=args.min_hits
        )


if __name__ == "__main__":
    main()
