#!/usr/bin/env python3
"""
äº¤é€šæ ‡å¿—æ£€æµ‹å™¨ - åŸºäºæ¨¡æ¿åŒ¹é…çš„äº¤é€šæ ‡å¿—æ£€æµ‹ç³»ç»Ÿ
å°†signsæ•°æ®é›†é›†æˆåˆ°SAM3å·¥ä½œæµä¸­

ç”¨æ³•:
    python3 scripts/traffic_sign_detector.py \
        --video data/D1_video_clips/your_video.mp4 \
        --output traffic_signs_detections.json \
        --signs-dir signs/highres/png2560px/
"""

import os
import sys
import json
import argparse
import cv2
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TrafficSignDetector:
    """äº¤é€šæ ‡å¿—æ£€æµ‹å™¨"""

    def __init__(self, signs_dir: str, threshold: float = 0.7):
        """
        åˆå§‹åŒ–äº¤é€šæ ‡å¿—æ£€æµ‹å™¨

        Args:
            signs_dir: äº¤é€šæ ‡å¿—å›¾åƒç›®å½•
            threshold: æ¨¡æ¿åŒ¹é…é˜ˆå€¼
        """
        self.signs_dir = Path(signs_dir)
        self.threshold = threshold
        self.sign_templates = {}
        self.sign_classes = {}

        # åŠ è½½æ‰€æœ‰äº¤é€šæ ‡å¿—æ¨¡æ¿
        self._load_sign_templates()

    def _load_sign_templates(self):
        """åŠ è½½äº¤é€šæ ‡å¿—æ¨¡æ¿"""
        logger.info(f"æ­£åœ¨åŠ è½½äº¤é€šæ ‡å¿—æ¨¡æ¿ä» {self.signs_dir}")

        if not self.signs_dir.exists():
            raise FileNotFoundError(f"äº¤é€šæ ‡å¿—ç›®å½•ä¸å­˜åœ¨: {self.signs_dir}")

        # æ”¯æŒçš„å›¾åƒæ ¼å¼
        image_extensions = {'.png', '.jpg', '.jpeg', '.bmp', '.tiff'}

        for img_path in self.signs_dir.glob('*'):
            if img_path.suffix.lower() in image_extensions:
                try:
                    # è¯»å–å›¾åƒ
                    img = cv2.imread(str(img_path), cv2.IMREAD_COLOR)
                    if img is None:
                        logger.warning(f"æ— æ³•è¯»å–å›¾åƒ: {img_path}")
                        continue

                    # è·å–æ ‡å¿—åç§°ï¼ˆå»é™¤æ–‡ä»¶æ‰©å±•åï¼‰
                    sign_name = img_path.stem

                    # å­˜å‚¨æ¨¡æ¿ä¿¡æ¯
                    self.sign_templates[sign_name] = {
                        'image': img,
                        'gray': cv2.cvtColor(img, cv2.COLOR_BGR2GRAY),
                        'size': img.shape[:2],
                        'filename': img_path.name
                    }

                    logger.info(f"åŠ è½½æ¨¡æ¿: {sign_name} ({img.shape})")

                except Exception as e:
                    logger.error(f"åŠ è½½æ¨¡æ¿å¤±è´¥ {img_path}: {e}")

        logger.info(f"æˆåŠŸåŠ è½½ {len(self.sign_templates)} ä¸ªäº¤é€šæ ‡å¿—æ¨¡æ¿")

    def _multi_scale_template_match(self, frame: np.ndarray, template: np.ndarray,
                                   sign_name: str) -> List[Dict[str, Any]]:
        """
        å¤šå°ºåº¦æ¨¡æ¿åŒ¹é…

        Args:
            frame: è¾“å…¥å¸§
            template: æ¨¡æ¿å›¾åƒ
            sign_name: æ ‡å¿—åç§°

        Returns:
            æ£€æµ‹ç»“æœåˆ—è¡¨
        """
        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        template_h, template_w = template.shape[:2]
        detections = []

        # å¤šå°ºåº¦èŒƒå›´ (0.2 åˆ° 2.0 å€)
        scales = np.linspace(0.2, 2.0, 15)

        for scale in scales:
            # ç¼©æ”¾æ¨¡æ¿
            scaled_w = int(template_w * scale)
            scaled_h = int(template_h * scale)

            if scaled_w > frame.shape[1] or scaled_h > frame.shape[0]:
                continue

            # ç¼©æ”¾æ¨¡æ¿
            scaled_template = cv2.resize(template, (scaled_w, scaled_h))

            # æ¨¡æ¿åŒ¹é…
            result = cv2.matchTemplate(frame_gray, scaled_template, cv2.TM_CCOEFF_NORMED)

            # æ‰¾åˆ°åŒ¹é…ä½ç½®
            locations = np.where(result >= self.threshold)

            for pt in zip(*locations[::-1]):  # åˆ‡æ¢ xå’Œy åæ ‡
                match_value = result[pt[1], pt[0]]

                detection = {
                    'bbox': [pt[0], pt[1], scaled_w, scaled_h],  # [x, y, w, h]
                    'confidence': float(match_value),
                    'class': sign_name,
                    'scale': scale
                }
                detections.append(detection)

        return detections

    def detect_frame(self, frame: np.ndarray, frame_idx: int = 0) -> List[Dict[str, Any]]:
        """
        æ£€æµ‹å•å¸§ä¸­çš„äº¤é€šæ ‡å¿—

        Args:
            frame: è¾“å…¥å¸§
            frame_idx: å¸§ç´¢å¼•

        Returns:
            æ£€æµ‹ç»“æœåˆ—è¡¨
        """
        all_detections = []

        # å¯¹æ¯ä¸ªæ¨¡æ¿è¿›è¡ŒåŒ¹é…
        for sign_name, template_info in self.sign_templates.items():
            detections = self._multi_scale_template_match(
                frame, template_info['gray'], sign_name
            )

            # æ·»åŠ å¸§ä¿¡æ¯
            for detection in detections:
                detection['frame'] = frame_idx
                detection['template_size'] = template_info['size']

            all_detections.extend(detections)

        # éæå¤§å€¼æŠ‘åˆ¶ (NMS)
        filtered_detections = self._apply_nms(all_detections)

        return filtered_detections

    def _apply_nms(self, detections: List[Dict[str, Any]],
                   nms_threshold: float = 0.4) -> List[Dict[str, Any]]:
        """
        åº”ç”¨éæå¤§å€¼æŠ‘åˆ¶å»é™¤é‡å æ£€æµ‹

        Args:
            detections: æ£€æµ‹ç»“æœåˆ—è¡¨
            nms_threshold: NMSé˜ˆå€¼

        Returns:
            è¿‡æ»¤åçš„æ£€æµ‹ç»“æœ
        """
        if not detections:
            return []

        # è½¬æ¢æ ¼å¼ç”¨äºNMS
        boxes = []
        scores = []
        classes = []

        for detection in detections:
            x, y, w, h = detection['bbox']
            boxes.append([x, y, x + w, y + h])
            scores.append(detection['confidence'])
            classes.append(detection['class'])

        boxes = np.array(boxes)
        scores = np.array(scores)

        # åº”ç”¨NMS
        indices = cv2.dnn.NMSBoxes(
            boxes.tolist(), scores.tolist(),
            self.threshold, nms_threshold
        )

        # è¿‡æ»¤æ£€æµ‹ç»“æœ
        if len(indices) > 0:
            indices = indices.flatten()
            filtered_detections = [detections[i] for i in indices]
        else:
            filtered_detections = []

        return filtered_detections

    def detect_video(self, video_path: str, output_path: str,
                    sample_rate: int = 5) -> Dict[str, Any]:
        """
        æ£€æµ‹è§†é¢‘ä¸­çš„äº¤é€šæ ‡å¿—

        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
            sample_rate: é‡‡æ ·ç‡ (æ¯Nå¸§å¤„ç†ä¸€æ¬¡)

        Returns:
            æ£€æµ‹ç»“æœ
        """
        logger.info(f"å¼€å§‹å¤„ç†è§†é¢‘: {video_path}")

        # æ‰“å¼€è§†é¢‘
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")

        # è·å–è§†é¢‘ä¿¡æ¯
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

        logger.info(f"è§†é¢‘ä¿¡æ¯: {frame_count}å¸§, {fps}FPS, {width}x{height}")

        all_detections = []
        frame_idx = 0

        # å¤„ç†è§†é¢‘å¸§
        while True:
            ret, frame = cap.read()
            if not ret:
                break

            # é‡‡æ ·å¤„ç†
            if frame_idx % sample_rate == 0:
                detections = self.detect_frame(frame, frame_idx)
                all_detections.extend(detections)

                if frame_idx % 50 == 0:
                    logger.info(f"å·²å¤„ç† {frame_idx}/{frame_count} å¸§, æ£€æµ‹åˆ° {len(detections)} ä¸ªæ ‡å¿—")

            frame_idx += 1

        cap.release()
        logger.info(f"è§†é¢‘å¤„ç†å®Œæˆ, å…±æ£€æµ‹åˆ° {len(all_detections)} ä¸ªäº¤é€šæ ‡å¿—")

        # è½¬æ¢ä¸ºSAM3æ ¼å¼
        sam3_results = self._convert_to_sam3_format(
            all_detections, video_path, fps, frame_count
        )

        # ä¿å­˜ç»“æœ
        self._save_results(sam3_results, output_path)

        return sam3_results

    def _convert_to_sam3_format(self, detections: List[Dict[str, Any]],
                               video_path: str, fps: float, frame_count: int) -> Dict[str, Any]:
        """
        å°†æ£€æµ‹ç»“æœè½¬æ¢ä¸ºSAM3/LabRL Studioæ ¼å¼

        Args:
            detections: æ£€æµ‹ç»“æœåˆ—è¡¨
            video_path: è§†é¢‘è·¯å¾„
            fps: å¸§ç‡
            frame_count: æ€»å¸§æ•°

        Returns:
            SAM3æ ¼å¼çš„ç»“æœ
        """
        # æŒ‰å¸§åˆ†ç»„æ£€æµ‹
        frame_detections = {}
        for detection in detections:
            frame_idx = detection['frame']
            if frame_idx not in frame_detections:
                frame_detections[frame_idx] = []
            frame_detections[frame_idx].append(detection)

        # åˆ›å»ºæ£€æµ‹ç»“æœJSON (å…¼å®¹ç°æœ‰SAM3æ ¼å¼)
        sam3_data = []

        # ç®€åŒ–çš„SAM3æ ¼å¼æ£€æµ‹ç»“æœ
        detection_results = {
            "video_info": {
                "path": video_path,
                "fps": fps,
                "frame_count": frame_count,
                "total_detections": len(detections)
            },
            "frames": {}
        }

        # æŒ‰å¸§ç»„ç»‡æ£€æµ‹ç»“æœ
        for frame_idx, frame_dets in frame_detections.items():
            frame_time = frame_idx / fps if fps > 0 else frame_idx

            detection_results["frames"][frame_idx] = {
                "timestamp": frame_time,
                "detections": []
            }

            for det in frame_dets:
                x, y, w, h = det['bbox']

                # è½¬æ¢ä¸ºç›¸å¯¹åæ ‡ (ç™¾åˆ†æ¯”)
                rel_x = (x / 100)  # å‡è®¾è¾“å…¥è§†é¢‘æ˜¯100å•ä½å®½ï¼Œéœ€è¦è°ƒæ•´
                rel_y = (y / 100)  # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…è§†é¢‘å°ºå¯¸è°ƒæ•´
                rel_width = (w / 100)
                rel_height = (h / 100)

                detection_entry = {
                    "bbox": det['bbox'],  # åŸå§‹åƒç´ åæ ‡
                    "confidence": det['confidence'],
                    "class": det['class'],
                    "frame": frame_idx,
                    "time": frame_time
                }

                detection_results["frames"][frame_idx]["detections"].append(detection_entry)

        # åˆ›å»ºLabel Studioå…¼å®¹æ ¼å¼
        label_studio_format = [{
            "data": {
                "video": f"/data/local-files/?d={Path(video_path).name}"
            },
            "predictions": [{
                "result": [],
                "score": 0.0
            }]
        }]

        # è¿™é‡Œå¯ä»¥è¿›ä¸€æ­¥æ·»åŠ è½¨è¿¹å¤„ç†é€»è¾‘
        # ç›®å‰å…ˆä¿å­˜åŸå§‹æ£€æµ‹ç»“æœ

        return {
            "detection_results": detection_results,
            "label_studio_format": label_studio_format,
            "raw_detections": detections
        }

    def _save_results(self, results: Dict[str, Any], output_path: str):
        """ä¿å­˜æ£€æµ‹ç»“æœ"""
        logger.info(f"ä¿å­˜ç»“æœåˆ°: {output_path}")

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        logger.info("ç»“æœä¿å­˜å®Œæˆ")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='äº¤é€šæ ‡å¿—æ£€æµ‹å™¨')
    parser.add_argument('--video', required=True, help='è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', required=True, help='è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--signs-dir', default='signs/highres/png2560px/',
                       help='äº¤é€šæ ‡å¿—å›¾åƒç›®å½•')
    parser.add_argument('--threshold', type=float, default=0.7,
                       help='æ¨¡æ¿åŒ¹é…é˜ˆå€¼')
    parser.add_argument('--sample-rate', type=int, default=5,
                       help='é‡‡æ ·ç‡ (æ¯Nå¸§å¤„ç†ä¸€æ¬¡)')
    parser.add_argument('--debug', action='store_true',
                       help='å¯ç”¨è°ƒè¯•æ¨¡å¼')

    args = parser.parse_args()

    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)

    try:
        # åˆ›å»ºæ£€æµ‹å™¨
        detector = TrafficSignDetector(args.signs_dir, args.threshold)

        # æ£€æµ‹è§†é¢‘
        results = detector.detect_video(
            args.video, args.output, args.sample_rate
        )

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        detection_results = results["detection_results"]
        total_detections = detection_results["video_info"]["total_detections"]
        frames_with_detections = len(detection_results["frames"])

        print(f"\nğŸ¯ æ£€æµ‹å®Œæˆ!")
        print(f"æ€»æ£€æµ‹æ•°: {total_detections}")
        print(f"æœ‰æ£€æµ‹çš„å¸§æ•°: {frames_with_detections}")
        print(f"ç»“æœä¿å­˜åˆ°: {args.output}")

    except Exception as e:
        logger.error(f"å¤„ç†å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()